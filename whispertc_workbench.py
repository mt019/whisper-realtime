# fw_streamlit.py
import streamlit as st
from faster_whisper import WhisperModel
from opencc import OpenCC
from pydub import AudioSegment
import io, os, time, wave, math, json, hashlib
import re
from typing import List, Tuple, Dict, Any, Pattern
from dataclasses import dataclass
import platform

from statistics import median
# --- multiprocessing context and traceback
from multiprocessing import get_context

from pathlib import Path

# Detect worker mode for child processes
IS_WORKER = os.environ.get("ASR_WORKER") == "1"

# --- Domain knowledge defaults ---
CONFIG_DEFAULT_PATH = Path(__file__).resolve().parent / "config" / "domain_defaults.json"


@st.cache_data(show_spinner=False)
def _load_defaults_config() -> Dict[str, Any]:
    candidates: List[Path] = []
    try:
        cfg_path = st.secrets.get("defaults_config_path")
        if cfg_path:
            if isinstance(cfg_path, (list, tuple)):
                for p in cfg_path:
                    candidates.append(Path(str(p)).expanduser())
            else:
                candidates.append(Path(str(cfg_path)).expanduser())
    except Exception:
        pass
    env_path = os.environ.get("WHISPERTC_DEFAULTS")
    if env_path:
        candidates.append(Path(env_path).expanduser())
    candidates.append(CONFIG_DEFAULT_PATH)

    for cand in candidates:
        try:
            if not cand.exists():
                continue
            data = json.loads(cand.read_text(encoding="utf-8"))
            return data if isinstance(data, dict) else {}
        except Exception:
            continue
    return {}


DEFAULTS_CONFIG = _load_defaults_config()

DOMAIN_SECRET_KEY = DEFAULTS_CONFIG.get("domain_secret_key", "domain_kb_paths")
DEFAULT_DOMAIN_PATHS = [str(Path(p).expanduser()) for p in DEFAULTS_CONFIG.get("domain_paths", [])]
DOMAIN_ALLOWED_SUFFIXES = set(DEFAULTS_CONFIG.get("allowed_suffixes", [".pdf", ".md", ".txt"]))
DOMAIN_IGNORE_KEYWORD = DEFAULTS_CONFIG.get("ignore_keyword", "ç¨¿")
DOMAIN_PRIORITY_HINTS = [str(h) for h in DEFAULTS_CONFIG.get("priority_hints", [])]

CORRECTION_SECRET_KEY = DEFAULTS_CONFIG.get("correction_secret_key", "correction_paths")
DEFAULT_CORRECTION_FILES: List[str] = [str(Path(p).expanduser()) for p in DEFAULTS_CONFIG.get("correction_files", [])]


def _extract_correction_map(cfg: Dict[str, Any]) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    # 1) support new correction_map format: {"correct": ["wrong1", ...]}
    corr_map = cfg.get("correction_map", {})
    if isinstance(corr_map, dict):
        for correct, wrongs in corr_map.items():
            if not correct:
                continue
            if isinstance(wrongs, (list, tuple, set)):
                for wrong in wrongs:
                    wrong = (wrong or "").strip()
                    if wrong:
                        mapping[wrong] = str(correct).strip()
            elif wrongs:
                mapping[str(wrongs).strip()] = str(correct).strip()
    # 2) backward compatibility for correction_pairs [[wrong, correct], ...]
    corr_pairs = cfg.get("correction_pairs", [])
    if isinstance(corr_pairs, (list, tuple)):
        for pair in corr_pairs:
            if isinstance(pair, (list, tuple)) and len(pair) == 2:
                wrong, correct = pair
                wrong = (wrong or "").strip()
                correct = (correct or "").strip()
                if wrong and correct:
                    mapping[wrong] = correct
    return mapping


DEFAULT_CORRECTION_MAP = _extract_correction_map(DEFAULTS_CONFIG)


@dataclass(frozen=True)
class PunctuationSettings:
    """Operator-tunable punctuation thresholds stored in milliseconds."""
    comma_ms: int
    period_ms: int
    adaptive: bool = True

    @property
    def comma_threshold(self) -> float:
        return max(0.01, self.comma_ms / 1000.0)

    @property
    def period_threshold(self) -> float:
        return max(0.01, self.period_ms / 1000.0)


def _load_default_punctuation_settings(cfg: Dict[str, Any]) -> PunctuationSettings:
    """Load punctuation thresholds from configuration with safe fallbacks."""
    base = cfg.get("punctuation_defaults", {}) if isinstance(cfg, dict) else {}

    def _int_field(key: str, default: int) -> int:
        try:
            return max(10, int(base.get(key, default)))
        except Exception:
            return default

    comma_ms = _int_field("comma_ms", 150)
    period_ms = _int_field("period_ms", 350)
    adaptive = bool(base.get("adaptive", True))
    return PunctuationSettings(comma_ms=comma_ms, period_ms=period_ms, adaptive=adaptive)


DEFAULT_PUNCTUATION_SETTINGS = _load_default_punctuation_settings(DEFAULTS_CONFIG)
CURRENT_PUNCT_SETTINGS: PunctuationSettings = DEFAULT_PUNCTUATION_SETTINGS


class StaticDomainFile:
    """Wrap local bytes payload to mimic Streamlit UploadedFile interface."""

    def __init__(self, name: str, data: bytes):
        self.name = name
        self._data = data

    def read(self) -> bytes:
        return self._data

    def seek(self, pos: int):
        # For API compatibility with UploadedFile
        return None


def _coerce_secret_paths(raw) -> List[str]:
    if not raw:
        return []
    if isinstance(raw, str):
        return [raw]
    if isinstance(raw, (list, tuple, set)):
        return [str(x) for x in raw if x]
    return []


@st.cache_data(show_spinner=False)
def _load_domain_bytes_from_paths(paths: Tuple[str, ...]) -> List[Tuple[str, bytes]]:
    collected: List[Tuple[str, bytes]] = []
    seen_digests: set[Tuple[str, str]] = set()
    for base in paths:
        base_path = Path(base).expanduser()
        if not base_path.exists():
            continue
        for file_path in base_path.rglob("*"):
            if not file_path.is_file():
                continue
            if DOMAIN_IGNORE_KEYWORD and DOMAIN_IGNORE_KEYWORD in file_path.as_posix():
                continue
            if file_path.suffix.lower() not in DOMAIN_ALLOWED_SUFFIXES:
                continue
            try:
                data = file_path.read_bytes()
                digest = hashlib.md5(data).hexdigest()
                # Avoid re-processing identical files that share the same name and content
                sig = (file_path.name.lower(), digest)
                if sig in seen_digests:
                    continue
                seen_digests.add(sig)
                collected.append((str(file_path), data))
            except Exception:
                continue
    return collected


def _resolve_default_domain_files() -> Tuple[List[StaticDomainFile], List[str]]:
    try:
        raw_paths = st.secrets.get(DOMAIN_SECRET_KEY)
    except Exception:
        raw_paths = None
    candidate_paths = _coerce_secret_paths(raw_paths)
    if not candidate_paths:
        candidate_paths = DEFAULT_DOMAIN_PATHS
    candidate_paths = [str(Path(p).expanduser()) for p in candidate_paths if p]
    if not candidate_paths:
        return [], []
    payload = _load_domain_bytes_from_paths(tuple(candidate_paths))
    files = [StaticDomainFile(name=path, data=data) for path, data in payload]
    return files, candidate_paths


preset_domain_files: List[StaticDomainFile] = []
preset_domain_paths: List[str] = []
combined_domain_files: List = []
CORRECTION_MAP: Dict[str, str] = {}


def _normalize_paths(paths: List[str]) -> Tuple[str, ...]:
    normalized = []
    for p in paths:
        if not p:
            continue
        normalized.append(str(Path(p).expanduser()))
    # å»é‡ä¸¦ä¿æŒé †åº
    seen = set()
    uniq = []
    for p in normalized:
        if p in seen:
            continue
        seen.add(p)
        uniq.append(p)
    return tuple(uniq)


@st.cache_data(show_spinner=False)
def _load_corrections_from_paths(paths: Tuple[str, ...], base_mapping: Dict[str, str]) -> Dict[str, str]:
    corrections: Dict[str, str] = dict(base_mapping)
    for p in paths:
        file_path = Path(p)
        if not file_path.exists() or not file_path.is_file():
            continue
        try:
            if file_path.suffix.lower() == ".json":
                data = json.loads(file_path.read_text(encoding="utf-8"))
                if isinstance(data, dict):
                    # Determine format: correct -> [wrongs] or wrong -> correct
                    if all(isinstance(v, (list, tuple, set)) for v in data.values()):
                        for correct, wrongs in data.items():
                            for wrong in wrongs:
                                w = (str(wrong) if wrong is not None else "").strip()
                                if w:
                                    corrections[w] = str(correct).strip()
                    else:
                        for wrong, correct in data.items():
                            w = (str(wrong) if wrong is not None else "").strip()
                            c = (str(correct) if correct is not None else "").strip()
                            if w and c:
                                corrections[w] = c
                elif isinstance(data, list):
                    for entry in data:
                        if isinstance(entry, dict):
                            wrong = (str(entry.get("wrong", "")).strip())
                            correct = (str(entry.get("correct", "")).strip())
                            if wrong and correct:
                                corrections[wrong] = correct
                            # support mapping style
                            for key, value in entry.items():
                                if key == "correct":
                                    continue
                                if key == "wrong":
                                    continue
                                if isinstance(value, (list, tuple, set)):
                                    for wrong in value:
                                        w = (str(wrong).strip())
                                        if w and entry.get("correct"):
                                            corrections[w] = str(entry.get("correct")).strip()
                        elif isinstance(entry, (list, tuple)) and len(entry) == 2:
                            wrong, correct = entry
                            w = (str(wrong).strip())
                            c = (str(correct).strip())
                            if w and c:
                                corrections[w] = c
                continue
            content = file_path.read_text(encoding="utf-8")
        except Exception:
            continue
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith("#") or line.startswith("//"):
                continue
            separator = None
            for cand in ("ï¼š", ":", "->", "=>", ",", "ï¼Œ"):
                if cand in line:
                    separator = cand
                    break
            if not separator:
                continue
            parts = line.split(separator, 1)
            if len(parts) != 2:
                continue
            wrong, right = parts[0].strip(), parts[1].strip()
            if wrong and right:
                corrections[wrong] = right
    return corrections


def _resolve_corrections(domain_dirs: Tuple[str, ...]) -> Dict[str, str]:
    try:
        raw_paths = st.secrets.get(CORRECTION_SECRET_KEY)
    except Exception:
        raw_paths = None
    candidate_paths = _coerce_secret_paths(raw_paths)
    # fallback to hardcoded + domain-local suggestion file
    fallback = list(DEFAULT_CORRECTION_FILES)
    for d in domain_dirs:
        fallback.append(str(Path(d) / "common_corrections.txt"))
    candidate_paths.extend(fallback)
    normalized = _normalize_paths(candidate_paths)
    return _load_corrections_from_paths(normalized, DEFAULT_CORRECTION_MAP)


def apply_common_corrections(text: str) -> str:
    if not text or not CORRECTION_MAP:
        return text
    output = text
    for wrong, right in CORRECTION_MAP.items():
        output = output.replace(wrong, right)
    return output


def _trigger_rerun():
    """Trigger a Streamlit rerun, compatible across versions."""
    rerun_fn = getattr(st, "experimental_rerun", None)
    if rerun_fn:
        rerun_fn()
        return
    rerun_fn = getattr(st, "rerun", None)
    if rerun_fn:
        rerun_fn()
        return
    try:
        from streamlit.runtime.scriptrunner import RerunException, RerunData
    except Exception as exc:
        raise RuntimeError("Streamlit rerun is not available in this environment.") from exc
    raise RerunException(RerunData())


def _on_start_transcribe():
    """Mark transcription as started and queue the main workflow."""
    if st.session_state.get("transcribing"):
        return
    st.session_state["transcribe_success_message"] = ""
    st.session_state["transcribing"] = True
    st.session_state["start_transcribe_pending"] = True
    st.session_state["transcribe_status_message"] = "ğŸš€ å·²æ”¶åˆ°è½‰éŒ„è«‹æ±‚ï¼Œæ­£åœ¨åˆå§‹åŒ–â€¦"


if not IS_WORKER:
    st.set_page_config(page_title="ç¨…æ³• STT", layout="wide")
    st.title("momo STTï½œèªéŸ³è½‰æ–‡å­—")

if not IS_WORKER:
    preset_domain_files, preset_domain_paths = _resolve_default_domain_files()
    CORRECTION_MAP = _resolve_corrections(tuple(preset_domain_paths))
    sidebar = st.sidebar
    sidebar.markdown(
        """
        <style>
        [data-testid="stSidebar"] .run-btn-wrapper button {
            font-size: 1.4rem;
            font-weight: 600;
            padding: 1.4rem 0.5rem;
        }
        [data-testid="stSidebar"] .run-btn-wrapper {
            padding-top: 0.5rem;
            padding-bottom: 0.5rem;
        }
        </style>
        <style>
        [data-testid="stFileUploaderDropzone"] {
            min-height: 12rem !important;  /* default is ~6rem, double it */
        }
        [data-testid="stFileUploaderDropzone"] section {
            padding: 2rem !important;      /* increase padding inside dropzone */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )
    with sidebar.container():
        sidebar.markdown('<div class="run-btn-wrapper">', unsafe_allow_html=True)
        if "transcribing" not in st.session_state:
            st.session_state["transcribing"] = False
        if "start_transcribe_pending" not in st.session_state:
            st.session_state["start_transcribe_pending"] = False
        if "transcribe_status_message" not in st.session_state:
            st.session_state["transcribe_status_message"] = ""
        if "transcribe_success_message" not in st.session_state:
            st.session_state["transcribe_success_message"] = ""
        sidebar.button(
            "é–‹å§‹è½‰å¯«",
            use_container_width=True,
            key="run_button_primary",
            on_click=_on_start_transcribe,
            disabled=st.session_state["transcribing"],
        )
        sidebar.markdown('</div>', unsafe_allow_html=True)






    sidebar.markdown("---")
    model_name = sidebar.selectbox("æ¨¡å‹",
        ["base","small","medium","distil-large-v3","large-v3"], index=2)
    compute = sidebar.selectbox("ç²¾åº¦", ["int8","float16","float32"], index=0)
    beam_size = sidebar.selectbox("æŸæœå°‹å¤§å°", [1,2,4,5,8], index=1)
    vad = sidebar.checkbox("å•Ÿç”¨ VAD éæ¿¾", True)
    with sidebar.expander("VAD éˆæ•åº¦", expanded=False):
        vad_threshold = st.slider("threshold", 0.05, 0.95, 0.50, 0.01)
        vad_min_silence = st.slider("min_silence_duration_ms", 50, 1200, 250, 10)
        vad_min_speech = st.slider("min_speech_duration_ms", 50, 800, 150, 10)
        vad_pad = st.slider("speech_pad_ms", 0, 500, 200, 10)
    punc_rule = sidebar.checkbox("ä¾åœé “è‡ªå‹•è£œæ¨™é»", True)
    default_punc = DEFAULT_PUNCTUATION_SETTINGS
    with sidebar.expander("æ¨™é»éˆæ•åº¦", expanded=False):
        comma_ms = st.slider("é€—è™Ÿé–€æª» (ms)", 80, 400, default_punc.comma_ms, 10)
        period_ms = st.slider("å¥è™Ÿé–€æª» (ms)", 120, 800, default_punc.period_ms, 10)
        adaptive_check = st.checkbox("è‡ªé©æ‡‰é–€æª»ï¼ˆä¾èªé€Ÿï¼‰", default_punc.adaptive)
    CURRENT_PUNCT_SETTINGS = PunctuationSettings(
        comma_ms=int(comma_ms),
        period_ms=int(period_ms),
        adaptive=bool(adaptive_check),
    )
    st.session_state["punctuation_settings"] = CURRENT_PUNCT_SETTINGS

    with sidebar.expander("æ®µè½è¼¸å‡º", expanded=False):
        reflow_enable = st.checkbox("ä¾èªå¢ƒåˆ†æ®µé¡¯ç¤º", False)
        reflow_width = st.slider("æ¯æ®µæœ€é•·å­—æ•¸", 20, 120, 60, 5)
    # éŸ³æª”æ˜¯å¦åˆ‡æˆå¤šæ®µè™•ç†ï¼ˆé è¨­ä¸åˆ‡ï¼Œé¿å…é¡å¤–éŒ¯èª¤é¢¨éšªï¼‰
    auto_chunk_audio = sidebar.checkbox("é•·éŒ„éŸ³è‡ªå‹•åˆ†æ®µè½‰å¯«", False)
    sidebar.markdown("---")
    sidebar.caption("å¯é¸ï¼šè¼‰å…¥ PDF/MD/TXT ä½œç‚ºé ˜åŸŸè©å½™ï¼Œå¢å¼·è¾¨è­˜")
    if preset_domain_files:
        sidebar.caption(f"é è¨­å·²æ›è¼‰ {len(preset_domain_files)} ä»½é ˜åŸŸæ–‡æœ¬ï¼ˆ{len(preset_domain_paths)} å€‹è³‡æ–™å¤¾ï¼‰")
        with sidebar.expander("è‡ªå‹•è¼‰å…¥ä¾†æº", expanded=False):
            st.write("ç›®éŒ„ï¼š")
            for p in preset_domain_paths:
                st.code(p)
            sample_names = [Path(f.name).name for f in preset_domain_files[:5]]
            if sample_names:
                st.write("ç¤ºä¾‹æª”æ¡ˆï¼š")
                st.code("\n".join(sample_names))
    domain_files = sidebar.file_uploader("é ˜åŸŸçŸ¥è­˜æª” (PDF/MD/TXT)", type=["pdf","md","txt"], accept_multiple_files=True)
    terms_topk = sidebar.slider("æœ€å¤šå¸¶å…¥è©å½™æ•¸", 10, 2000, 1500, 10)
    auto_use_domain = sidebar.checkbox("åœ¨è½‰å¯«æ™‚è‡ªå‹•å¸¶å…¥é ˜åŸŸè©å½™", True)

    init_prompt = sidebar.text_input("åˆå§‹æç¤ºï¼ˆå¯ç•™ç©ºï¼‰", "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚")
    combined_domain_files = list(preset_domain_files)
    if domain_files:
        combined_domain_files.extend(domain_files)

cc = OpenCC("s2t")

# The following parameters must be available in both UI and worker mode
if not IS_WORKER:
    vad_params = {
        "threshold": vad_threshold,
        "min_silence_duration_ms": vad_min_silence,
        "min_speech_duration_ms": vad_min_speech,
        "speech_pad_ms": vad_pad,
    }
else:
    # Dummy values for workers (actual values passed as arguments)
    vad_params = {}
    CORRECTION_MAP = dict(DEFAULT_CORRECTION_MAP)
    CURRENT_PUNCT_SETTINGS = DEFAULT_PUNCTUATION_SETTINGS



@st.cache_resource(show_spinner=False)
def load_model(name, compute_type):
    # å˜—è©¦ä½¿ç”¨ Metalï¼Œä¸æ”¯æ´æ™‚è‡ªå‹•å›é€€åˆ° CPUï¼Œä¸¦è¨˜éŒ„å¯¦éš›è£ç½®
    for device in ("metal", "cpu"):
        try:
            cpu_threads = 1 if device == "metal" else 0
            m = WhisperModel(name, device=device, compute_type=compute_type, cpu_threads=cpu_threads)
            st.session_state["device_used"] = device
            return m
        except Exception:
            continue
    # æœ€å¾Œé˜²ç¦¦ï¼šå¼·åˆ¶ CPU
    m = WhisperModel(name, device="cpu", compute_type=compute_type, cpu_threads=0)
    st.session_state["device_used"] = "cpu"
    return m

def fmt_dur(sec: float) -> str:
    sec = max(0, int(round(sec)))
    h = sec // 3600
    m = (sec % 3600) // 60
    s = sec % 60
    return f"{h:02d}:{m:02d}:{s:02d}"


CJK_PUNCS = "ï¼Œã€‚ã€ï¼Ÿï¼ï¼šï¼›,.!?"

ZH_PUNC_MAP = str.maketrans({
    ",": "ï¼Œ",
    ".": "ã€‚",
    "?": "ï¼Ÿ",
    "!": "ï¼",
    ":": "ï¼š",
    ";": "ï¼›",
})

def normalize_zh_punc(s: str) -> str:
    if not s:
        return s
    # æ›¿æ›åŠå½¢è‹±å¼æ¨™é»ç‚ºå…¨å½¢ä¸­æ–‡æ¨™é»
    s = s.translate(ZH_PUNC_MAP)
    # ç§»é™¤æ¨™é»å‰å¤šé¤˜ç©ºç™½
    for p in "ï¼Œã€‚ï¼Ÿï¼ï¼šï¼›":
        s = s.replace(" " + p, p)
    return s

# --- ç°¡æ˜“æ–‡æœ¬æŠ½å–èˆ‡è©å½™æŠ½å– ---
def _read_pdf_bytes(name: str, data: bytes) -> str:
    try:
        # è¼•ä¾è³´ï¼špypdfï¼Œå¦‚ç’°å¢ƒæœªå®‰è£ï¼Œå›é€€ç©ºå­—ä¸²
        from pypdf import PdfReader  # type: ignore
        reader = PdfReader(io.BytesIO(data))
        texts = []
        for p in reader.pages:
            try:
                t = p.extract_text() or ""
            except Exception:
                t = ""
            if t:
                texts.append(t)
        return "\n".join(texts)
    except Exception:
        return ""

_MD_CODE_FENCE = re.compile(r"```[\s\S]*?```", re.MULTILINE)
_MD_INLINE_CODE = re.compile(r"`[^`]+`")
_MD_LINK = re.compile(r"\[([^\]]+)\]\([^\)]+\)")
_MD_IMAGE = re.compile(r"!\[[^\]]*\]\([^\)]+\)")
_HTML_TAG = re.compile(r"<[^>]+>")

def _md_to_text(s: str) -> str:
    s = _MD_CODE_FENCE.sub(" ", s)
    s = _MD_IMAGE.sub(" ", s)
    s = _MD_LINK.sub(r"\1", s)
    s = _MD_INLINE_CODE.sub(" ", s)
    # å»é™¤æ¨™é¡Œ/æ¸…å–®ç¬¦è™Ÿ
    s = re.sub(r"^[#>*\-+\s]+", "", s, flags=re.MULTILINE)
    s = _HTML_TAG.sub(" ", s)
    s = re.sub(r"\|", " ", s)  # è¡¨æ ¼åˆ†éš”
    s = re.sub(r"\s+", " ", s)
    return s.strip()

_RE_CJK = re.compile(r"[\u4e00-\u9fff]{2,10}")
_RE_EN = re.compile(r"[A-Za-z][A-Za-z0-9_\-]{2,}")

def extract_terms(text: str, top_k: int = 60) -> List[str]:
    if not text:
        return []
    counts: dict[str, int] = {}
    # CJK ç‰‡èªï¼ˆé€£çºŒæ¼¢å­— 2~10ï¼‰
    for m in _RE_CJK.finditer(text):
        tok = m.group(0)
        counts[tok] = counts.get(tok, 0) + 1
    # è‹±æ–‡/ä»£è™Ÿï¼ˆä¿ç•™å¤§å°å¯«çš„ä»£è¡¨å½¢å¼ï¼Œçµ±è¨ˆä»¥å°å¯«ï¼‰
    seen_case: dict[str, str] = {}
    for m in _RE_EN.finditer(text):
        raw = m.group(0)
        key = raw.lower()
        if key not in seen_case:
            seen_case[key] = raw
        counts[key] = counts.get(key, 0) + 1
    # æ’åºï¼šé »ç‡ã€é•·åº¦
    items = []
    for k, v in counts.items():
        shown = seen_case.get(k, k)
        items.append((v, len(shown), shown))
    items.sort(key=lambda x: (-x[0], -x[1], x[2]))
    terms = []
    for _, _, w in items:
        if w not in terms:
            terms.append(w)
        if len(terms) >= max(10, top_k):
            break
    return terms

def build_domain_prompt(files, top_k: int = 60) -> tuple[str, List[str]]:
    texts = []
    for f in files or []:
        name = getattr(f, "name", "") or ""
        data = f.read()
        if hasattr(f, "seek"):
            try:
                f.seek(0)
            except Exception:
                pass
        try:
            if name.lower().endswith(".pdf"):
                t = _read_pdf_bytes(name, data)
            elif name.lower().endswith(".md"):
                t = _md_to_text(data.decode("utf-8", errors="ignore"))
            else:
                t = data.decode("utf-8", errors="ignore")
        except Exception:
            t = ""
        if t:
            weight = 1
            for hint in DOMAIN_PRIORITY_HINTS:
                if hint and hint in name:
                    # Boost repeated text so priority files surface more terms
                    weight = 5
                    break
            texts.extend([t] * weight)
    big = "\n".join(texts)
    terms = extract_terms(big, top_k=top_k)
    # ä»¥å…¨å½¢é€—è™Ÿåˆ†éš”ï¼Œæ§åˆ¶é•·åº¦ï¼ˆé¿å…éé•· promptï¼‰
    prompt = "ï¼Œ".join(terms)
    if len(prompt) > 800:
        prompt = prompt[:800]
    return prompt, terms

def count_term_hits(text: str, terms: List[str]) -> tuple[int, int]:
    if not text or not terms:
        return 0, 0
    uniq = 0
    total = 0
    t_low = text.lower()
    for t in terms:
        if not t:
            continue
        # è‹±æ–‡ç”¨ä¸åˆ†å¤§å°å¯«ï¼›CJK ç›´æ¥æ¯”å°
        if re.search(r"[A-Za-z]", t):
            c = t_low.count(t.lower())
        else:            
            c = text.count(t)
        if c > 0:
            uniq += 1
            total += c
    return uniq, total

# æ®µè½é‡æ’
def reflow_text(s: str, max_len: int = 60) -> str:
    if not s:
        return s
    # å…ˆåœ¨å¥æœ«æ¨™é»å¾ŒåŠ æ®µè½
    out = s
    for p in ("ã€‚", "ï¼", "ï¼Ÿ"):
        out = out.replace(p, p + "\n\n")
    # åŸºæ–¼é•·åº¦çš„ç°¡å–®æ›è¡Œï¼šæ¯æ®µè¶…é max_len ä¸”æœ«å°¾ç‚ºé€—è™Ÿæ™‚æ›è¡Œ
    lines = []
    for para in out.split("\n\n"):
        buf = ""
        for ch in para:
            buf += ch
            if len(buf) >= max_len and buf.endswith("ï¼Œ"):
                lines.append(buf.strip())
                buf = ""
        if buf:
            lines.append(buf.strip())
    return "\n\n".join(lines)





def typical_gap_from_words(words) -> float:
    gaps = []
    prev_e = None
    for w in words:
        if prev_e is not None and w.start is not None and prev_e is not None:
            g = (w.start or 0.0) - (prev_e or 0.0)
            if g > 0.03:  # å¿½ç•¥æ¥µçŸ­æ•¸å€¼æŠ–å‹•
                gaps.append(g)
        prev_e = w.end or prev_e
    return median(gaps) if gaps else 0.0

def save_wav(bytes_data, path):
    with wave.open(path, "wb") as w:
        w.setnchannels(1); w.setsampwidth(2); w.setframerate(44100)
        w.writeframes(bytes_data)

def audio_info_from_bytes(b: bytes, filename: str|None=None) -> Tuple[AudioSegment, float]:
    buf = io.BytesIO(b)
    fmt = None
    if filename and "." in filename:
        fmt = filename.rsplit(".",1)[-1].lower()
    seg = AudioSegment.from_file(buf, format=fmt)  # äº¤çµ¦ pydub/ffmpeg åˆ¤æ–·
    dur = seg.duration_seconds
    return seg, dur

def export_seg_to_wav(seg: AudioSegment, path: str):
    seg.export(path, format="wav")  # äº¤çµ¦ ffmpeg

def start_timecode(t: float) -> str:
    return time.strftime("%H:%M:%S", time.gmtime(max(t, 0))) + f",{int((t%1)*1000):03d}"

def to_seconds(ts: str) -> float:
    """å°‡ SRT æ™‚é–“ç¢¼ HH:MM:SS,mmm è½‰ç‚ºç§’æ•¸"""
    try:
        h, m, s_ms = ts.split(":")
        s, ms = s_ms.split(",")
        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0
    except Exception:
        return 0.0
    

@dataclass(frozen=True)
class LexicalRules:
    sentence_starters: frozenset[str]
    clause_starters: frozenset[str]
    clause_pause_after: frozenset[str]
    continuation_words: frozenset[str]
    sentence_break_after: frozenset[str]
    question_endings: str
    enum_token_re: Pattern[str]
    no_break_suffixes: tuple[str, ...]
    sentence_break_tails: tuple[str, ...]
    clause_break_before: tuple[str, ...]


def _normalize_token_list(raw: Any) -> List[str]:
    if not raw:
        return []
    if isinstance(raw, str):
        raw = raw.split(",")
    if isinstance(raw, (list, tuple, set)):
        out = []
        for item in raw:
            token = str(item).strip()
            if token:
                out.append(token)
        return out
    return []


DEFAULT_LEXICAL_RULES = LexicalRules(
    sentence_starters=frozenset(
        {
            "å› æ­¤", "æ‰€ä»¥", "ç„¶è€Œ", "ä½†æ˜¯", "ä¸é", "å¯æ˜¯", "çµæœ",
            "æœ€å¾Œ", "æœ€å¾Œä¸€å€‹", "æœ€å¾Œé¢", "æ¥ä¸‹ä¾†", "ç„¶å¾Œ", "å†ä¾†", "å¦å¤–",
            "æ­¤å¤–", "é™¤æ­¤ä¹‹å¤–", "åŒæ™‚", "ç¸½ä¹‹", "ç¸½è€Œè¨€ä¹‹",
        }
    ),
    clause_starters=frozenset(
        {
            "å› ç‚º", "å¦‚æœ", "ç•¶", "ç•¶ç„¶", "å°¤å…¶", "å°¤å…¶æ˜¯", "ç‰¹åˆ¥æ˜¯",
            "ä¾‹å¦‚", "æ¯”å¦‚", "æ¯”å¦‚èªª", "èˆ‰ä¾‹ä¾†èªª", "æ›å¥è©±èªª", "ä¹Ÿå°±æ˜¯èªª",
            "æˆ–è€…", "æˆ–è€…æ˜¯", "ä»¥åŠ", "é‚„æœ‰", "å¦å¤–", "å†åŠ ä¸Š", "åŒæ™‚",
            "åŒ…å«", "åŒ…æ‹¬",
        }
    ),
    clause_pause_after=frozenset(
        {
            "ä¾‹å¦‚", "æ¯”å¦‚", "æ¯”å¦‚èªª", "èˆ‰ä¾‹ä¾†èªª", "æ›å¥è©±èªª", "ä¹Ÿå°±æ˜¯èªª",
            "ç¸½ä¹‹", "ç¸½è€Œè¨€ä¹‹", "æ‰€ä»¥", "å› æ­¤", "å› ç‚º", "çµæœ",
        }
    ),
    continuation_words=frozenset(
        {
            "å’Œ", "è·Ÿ", "èˆ‡", "åŠ", "ä¸¦ä¸”", "è€Œä¸”", "ä»¥åŠ", "é‚„æœ‰", "é‚„è¦", "é‚„æœƒ", "é‚„æœƒå†", "å†", "åˆ",
            "åŒæ™‚", "æˆ–è€…", "æˆ–æ˜¯", "è€Œ", "è€Œä¸”åœ¨", "ä¸¦ä¸”åœ¨", "è€Œåœ¨", "é‚„åœ¨",
        }
    ),
    sentence_break_after=frozenset(
        {
            "å› æ­¤", "æ‰€ä»¥", "ç¸½ä¹‹", "ç¸½è€Œè¨€ä¹‹", "çµæœ", "æ›å¥è©±èªª", "ä¹Ÿå°±æ˜¯èªª", "å› è€Œ", "å› æ­¤åœ¨",
            "å› æ­¤å°±", "æ‰€ä»¥å°±", "å› æ­¤æ‰",
        }
    ),
    question_endings="å—å‘¢ï¼Ÿ?",
    enum_token_re=re.compile(r"^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€ï¼.]?$"),
    no_break_suffixes=(
        "çš„", "å¾—", "åœ°", "è‘—", "ç€", "äº†", "å‘¢", "å—", "å˜›", "å•Š", "å‘€", "å•¦", "å§", "å–”", "å“¦", "å™¢",
    ),
    sentence_break_tails=(
        "çš„æ™‚å€™", "çš„éšæ®µ", "çš„æƒ…æ³ä¸‹", "çš„çµæœ", "çš„ç‹€æ³ä¸‹", "ä¹‹ä¸‹", "ä¹‹å‰", "ä¹‹å¾Œ", "ä¹‹æ™‚", "ä¹‹éš›",
    ),
    clause_break_before=(
        "å¦‚æœ", "å‡å¦‚", "ç•¶", "ç•¶ä½ ", "ç•¶æˆ‘å€‘", "ç•¶ç„¶", "å°¤å…¶", "å°¤å…¶æ˜¯", "ç‰¹åˆ¥æ˜¯",
        "å¦å¤–", "æ­¤å¤–", "é‚„æœ‰", "ä»¥åŠ", "å†åŠ ä¸Š", "åŒæ™‚", "ç”šè‡³", "ç”šè‡³æ–¼", "æˆ–è€…", "æˆ–æ˜¯",
        "ä¾‹å¦‚", "æ¯”å¦‚", "æ¯”å¦‚èªª", "èˆ‰ä¾‹ä¾†èªª", "æ›å¥è©±èªª", "ä¹Ÿå°±æ˜¯èªª",
    ),
)


def _load_lexical_rules(base: LexicalRules, overrides: Dict[str, Any] | None) -> LexicalRules:
    if not overrides or not isinstance(overrides, dict):
        return base

    def merge_set(key: str, current: frozenset[str]) -> frozenset[str]:
        tokens = _normalize_token_list(overrides.get(key))
        return current if not tokens else frozenset(tokens)

    def merge_tuple(key: str, current: tuple[str, ...]) -> tuple[str, ...]:
        tokens = _normalize_token_list(overrides.get(key))
        return current if not tokens else tuple(tokens)

    qe_raw = overrides.get("question_endings")
    question_endings = base.question_endings
    if isinstance(qe_raw, str) and qe_raw.strip():
        question_endings = qe_raw.strip()

    enum_pattern_raw = overrides.get("enum_token_pattern")
    enum_token_re = base.enum_token_re
    if isinstance(enum_pattern_raw, str) and enum_pattern_raw.strip():
        try:
            enum_token_re = re.compile(enum_pattern_raw.strip())
        except re.error:
            pass

    return LexicalRules(
        sentence_starters=merge_set("sentence_starters", base.sentence_starters),
        clause_starters=merge_set("clause_starters", base.clause_starters),
        clause_pause_after=merge_set("clause_pause_after", base.clause_pause_after),
        continuation_words=merge_set("continuation_words", base.continuation_words),
        sentence_break_after=merge_set("sentence_break_after", base.sentence_break_after),
        question_endings=question_endings,
        enum_token_re=enum_token_re,
        no_break_suffixes=merge_tuple("no_break_suffixes", base.no_break_suffixes),
        sentence_break_tails=merge_tuple("sentence_break_tails", base.sentence_break_tails),
        clause_break_before=merge_tuple("clause_break_before", base.clause_break_before),
    )


LEXICAL_RULES = _load_lexical_rules(
    DEFAULT_LEXICAL_RULES,
    DEFAULTS_CONFIG.get("punctuation_rules"),
)


class SmartPunctuator:
    """Rule-based punctuation controller combining timing gaps and lexical heuristics."""

    def __init__(self, rules: LexicalRules | None = None) -> None:
        self.rules = rules or LEXICAL_RULES
        self.tokens: List[str] = []
        self.prev_end: float | None = None
        self.sentence_chars: int = 0
        self.clause_chars: int = 0
        self.prev_word: str = ""

    def _char_len(self, text: str) -> int:
        return sum(1 for ch in text if ch not in CJK_PUNCS and not ch.isspace())

    def _after_punc(self, punc: str) -> None:
        if punc in ("ã€‚", "ï¼", "ï¼Ÿ"):
            self.sentence_chars = 0
            self.clause_chars = 0
        elif punc in ("ï¼Œ", "ã€", "ï¼›", "ï¼š"):
            self.clause_chars = 0

    def _append_to_last(self, punc: str) -> None:
        if not self.tokens:
            return
        last = self.tokens[-1].rstrip()
        if not last:
            return
        last_tail = last[-1]
        if last_tail in "ã€‚ï¼ï¼Ÿ" and punc in "ã€‚ï¼ï¼Ÿ":
            return
        if last_tail in "ï¼Œã€ï¼›ï¼š" and punc in "ï¼Œã€ï¼›ï¼š":
            return
        if last_tail in "ï¼Œã€ï¼›ï¼š" and punc in "ã€‚ï¼ï¼Ÿ":
            last = last.rstrip("ï¼Œã€ï¼›ï¼š")
        self.tokens[-1] = last + punc
        self._after_punc(punc)
        self.prev_word = self.tokens[-1]

    def _push_word(self, text: str) -> None:
        text = text.strip()
        if not text:
            return
        self.tokens.append(text)
        char_len = self._char_len(text)
        self.sentence_chars += char_len
        self.clause_chars += char_len
        tail = text[-1]
        if tail in "ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š":
            self._after_punc(tail)
        self.prev_word = text

    def _choose_sentence_end(self) -> str:
        last = self.prev_word.strip()
        if last and last[-1] in self.rules.question_endings:
            return "ï¼Ÿ"
        return "ã€‚"

    def _decide_punc(self, gap: float, upcoming: str, comma_th: float, period_th: float, coarse: bool) -> str:
        if not self.tokens:
            return ""
        gap = max(0.0, gap)
        sentence_score = 0.0
        comma_score = 0.0
        upcoming = upcoming.strip()
        prev = self.prev_word.strip()

        # æ™‚é–“é–“éš”æ¬Šé‡
        if gap >= period_th:
            sentence_score += 4.5
        elif gap >= period_th * 0.9:
            sentence_score += 3.2
        elif gap >= max(period_th * 0.7, comma_th * 1.3):
            sentence_score += 1.4

        if gap >= comma_th:
            comma_score += 3.0
        elif gap >= comma_th * 0.85:
            comma_score += 2.0
        elif gap >= comma_th * 0.7:
            comma_score += 0.8

        # å¥é•·/å­å¥é•·æ¬Šé‡
        if self.sentence_chars >= 52:
            sentence_score += 4.0
        elif self.sentence_chars >= 40:
            sentence_score += 2.4
        elif self.sentence_chars >= 28 and self.clause_chars >= 16:
            sentence_score += 1.2

        if self.clause_chars >= 28:
            comma_score += 2.6
        elif self.clause_chars >= 20:
            comma_score += 1.8
        elif self.clause_chars >= 14:
            comma_score += 1.0

        lexical_force = ""

        # è©å½™ heuristics
        if upcoming:
            if upcoming in self.rules.sentence_starters and self.sentence_chars >= 12:
                sentence_score += 2.6
            if upcoming in self.rules.clause_starters and self.clause_chars >= 6:
                comma_score += 2.4
            if upcoming in self.rules.clause_pause_after and self.clause_chars >= 6:
                comma_score += 1.4
            if self.rules.enum_token_re.match(upcoming) and self.sentence_chars >= 8:
                sentence_score += 2.2
            if any(upcoming.startswith(pat) for pat in self.rules.clause_break_before):
                lexical_force = "comma"
                comma_score += 2.0

        if prev:
            trimmed_prev = prev.rstrip("ï¼Œã€ï¼›ï¼š")
            if trimmed_prev[-1:] in self.rules.question_endings:
                sentence_score += 1.0
            if trimmed_prev in self.rules.clause_pause_after:
                comma_score += 1.6
            if trimmed_prev in self.rules.sentence_starters:
                sentence_score += 1.2
            if trimmed_prev.endswith(("ä½†æ˜¯", "ç„¶è€Œ", "å¯æ˜¯")):
                sentence_score += 1.4
            if any(trimmed_prev.endswith(pat) for pat in self.rules.sentence_break_tails):
                sentence_score += 2.5
                lexical_force = lexical_force or "comma"
            if trimmed_prev in self.rules.sentence_break_after:
                lexical_force = "period"

        # ç²—ç²’åº¦è¼¸å…¥æ™‚æ¸›å°‘ä¿¡å¿ƒ
        if coarse:
            comma_score *= 0.85
            sentence_score *= 0.85

        # é¿å…éçŸ­å­å¥æ’å…¥ï¼ŒåŒæ™‚ä¿ç•™é©é‡æ¨™é»
        if self.clause_chars < 6:
            comma_score *= 0.55
            sentence_score *= 0.7

        # é‡åˆ°èªåŠ©è©æˆ–é€£æ¥è©æ™‚é™ä½ä¿¡å¿ƒï¼Œé¿å…åœ¨å¥ä¸­æ–·å¥
        def _should_penalize(word: str) -> bool:
            if not word:
                return False
            w = word.strip().rstrip("ï¼Œã€ï¼›ï¼š")
            if not w:
                return False
            if len(w) <= 1 and w not in self.rules.question_endings:
                return True
            if w in self.rules.continuation_words:
                return True
            if w.endswith(self.rules.no_break_suffixes):
                return True
            return False

        penal_prev = _should_penalize(prev)
        penal_next = _should_penalize(upcoming)
        if penal_prev:
            comma_score *= 0.6
            sentence_score *= 0.75

        if penal_next:
            comma_score *= 0.7
            sentence_score *= 0.8

        if penal_prev and penal_next:
            comma_score *= 0.65
            sentence_score *= 0.7

        if lexical_force == "period" and gap >= max(comma_th * 0.4, 0.35):
            sentence_score += 3.0
        elif lexical_force == "comma" and gap >= max(comma_th * 0.3, 0.25):
            comma_score += 1.8

        if sentence_score >= max(3.5, comma_score + 0.8):
            return "period"
        if comma_score >= max(2.2, sentence_score + 0.3):
            return "comma"
        return ""



    def _maybe_insert(self, gap: float, upcoming: str, comma_th: float, period_th: float, coarse: bool) -> None:
        """
        æ ¹æ“šæ™‚é–“é–“éš”èˆ‡èªå¢ƒæ±ºå®šæ˜¯å¦åœ¨å‰ä¸€å€‹ token å¾Œæ’å…¥æ¨™é»ã€‚
        - è‹¥ gap <= 0ï¼šå¼·åˆ¶åŠ é€—è™Ÿï¼Œé¿å…å¥å­ç·Šæ¥æˆ–é‡ç–Šæ™‚ç„¡æ–·é»ã€‚
        - å…¶ä»–æƒ…æ³ï¼šäº¤ç”± _decide_punc() åˆ¤æ–·å¥è™Ÿï¼é€—è™Ÿã€‚
        """
        # è‹¥ gap ç‚ºè² æˆ–é›¶ï¼ˆé‡ç–Šã€ç·Šæ¥ï¼‰ï¼Œä»å¼·åˆ¶æ’å…¥é€—è™Ÿ
        if gap <= 0 and self.tokens:
            self._append_to_last("ï¼Œ")
            return

        # ä¸€èˆ¬æ±ºç­–é‚è¼¯
        decision = self._decide_punc(gap, upcoming, comma_th, period_th, coarse)
        if not decision:
            return

        if decision == "period":
            punc = self._choose_sentence_end()
        else:
            punc = "ï¼Œ"

        self._append_to_last(punc)

    def add_word(self, word: str, start: float | None, end: float | None, comma_th: float, period_th: float) -> None:
        word = word.strip()
        if not word:
            return
        if self.prev_end is None:
            gap = float(start or 0.0)
        else:
            gap = float((start or 0.0) - (self.prev_end or 0.0))
        self._maybe_insert(gap, word, comma_th, period_th, coarse=False)
        self._push_word(word)
        if end is not None:
            self.prev_end = float(end)
        elif self.prev_end is None:
            self.prev_end = float(start or 0.0)

    def add_chunk(self, text: str, start: float | None, end: float | None, comma_th: float, period_th: float) -> None:
        """å®‰å…¨æ”¯æ´ None æ™‚é–“è¼¸å…¥"""
        text = text.strip()
        if not text:
            return
        s_val = float(start or 0.0)
        e_val = float(end or s_val)
        gap = s_val - (self.prev_end or 0.0) if self.prev_end is not None else s_val
        self._maybe_insert(gap, text, comma_th, period_th, coarse=True)
        self._push_word(text)
        self.prev_end = e_val

    def ensure_terminal(self) -> None:
        if not self.tokens:
            return
        last = self.tokens[-1].rstrip()
        if not last:
            return
        if last[-1] in "ã€‚ï¼ï¼Ÿ":
            return
        self.tokens[-1] = last + self._choose_sentence_end()
        self._after_punc(self.tokens[-1][-1])



def transcribe_one(
    media_path: str,
    model: WhisperModel,
    language: str,
    vad_filter: bool,
    beam_size: int,
    initial_prompt: str | None,
    punc_rule: bool,
    vad_parameters: dict | None,
    ui_area,
    progress_area,
    stats_area,
    time_offset_sec: float = 0.0,
    total_sec_for_progress: float | None = None,
    lexical_rules: LexicalRules | None = None,
    punc_settings: PunctuationSettings | None = None,
):
    """å–®æ®µè½‰éŒ„ï¼ŒSRT ç„¡æ¨™é»ï¼Œå³æ™‚é¡¯ç¤ºç‚ºåŸå§‹å°å¥ï¼Œæœ€çµ‚æ•´åˆä½¿ç”¨ SmartPunctuator è£œæ¨™é»ã€‚"""
    live_box = ui_area.empty()
    acc = []  # æ”¶é›†åŸå§‹å°å¥
    prev_end = 0.0
    srt_lines, idx = [], 1
    t0 = time.time()
    processed_sec = 0.0
    prog = progress_area.progress(0.0, text="åˆ†æ®µè™•ç†ä¸­â€¦")
    total_sec = total_sec_for_progress or 1.0
    lexical_cfg = lexical_rules or LEXICAL_RULES
    punctuation_cfg = punc_settings or CURRENT_PUNCT_SETTINGS


    # === è½‰éŒ„ ===
    segments, info = model.transcribe(
        media_path,
        language=language,
        vad_filter=vad_filter,
        beam_size=beam_size,
        temperature=0.2,
        # best_of=5,
        condition_on_previous_text=True,
        task="transcribe",
        initial_prompt=initial_prompt or None,
        word_timestamps=True,
        vad_parameters=vad_parameters,
    )

    displayed_idx = 0
    for seg in segments:
        words = getattr(seg, "words", None)
        c_th = punctuation_cfg.comma_threshold
        p_th = punctuation_cfg.period_threshold
        adaptive_enabled = punctuation_cfg.adaptive and punc_rule
        if adaptive_enabled and words:
            tg = typical_gap_from_words(words)
            if tg > 0:
                c_th = max(0.10, min(c_th, 0.60 * tg))
                p_th = max(c_th + 0.03, min(p_th, 1.20 * tg))

        # --- ä¸åœ¨å³æ™‚éšæ®µåŠ æ¨™é»ï¼Œåªæ”¶é›†ç´”æ–‡æœ¬ ---
        raw_text = normalize_zh_punc(cc.convert(seg.text)).strip()
        if not raw_text:
            continue
        acc.append(raw_text)
        prev_end = seg.end or prev_end

        # --- å¯«å…¥ SRTï¼ˆç„¡æ¨™é»å°å¥ï¼‰ ---
        adj_start = (seg.start or 0.0) + time_offset_sec
        adj_end = (seg.end or 0.0) + time_offset_sec
        srt_lines.append(f"{idx}")
        srt_lines.append(f"{start_timecode(adj_start)} --> {start_timecode(adj_end)}")
        srt_lines.append(apply_common_corrections(raw_text))  # ç„¡æ¨™é»
        srt_lines.append("")
        idx += 1

        # --- å³æ™‚é¡¯ç¤ºï¼šæ‰€æœ‰å·²ç”Ÿæˆå°å¥ï¼Œç„¡é‡è¤‡ ---
        all_lines = [
            apply_common_corrections(line.strip())
            for i, line in enumerate(srt_lines)
            if i % 4 == 2 and line.strip()
        ]
        new_lines = all_lines[displayed_idx:]
        if new_lines:
            displayed_idx = len(all_lines)
            cur_display_text = "  \n".join(all_lines)
            live_box.markdown(cur_display_text)

            # âºï¸ Incremental save during transcription
            txt_path = st.session_state.get("current_txt_path")
            srt_path = st.session_state.get("current_srt_path")
            if txt_path and srt_path:
                # Write to TXT
                with open(txt_path, "a", encoding="utf-8") as f_txt:
                    f_txt.write(raw_text + "\n")
                # Write full SRT block
                with open(srt_path, "a", encoding="utf-8") as f_srt:
                    f_srt.write(
                        srt_lines[-4] + "\n" +  # index
                        srt_lines[-3] + "\n" +  # timecode
                        srt_lines[-2] + "\n\n"  # text + blank
                    )

        # --- æ›´æ–°é€²åº¦ ---
        processed_sec = max(processed_sec, float(seg.end or 0.0))
        elapsed = time.time() - t0
        eta_txt = ""
        if processed_sec > 5 and elapsed > 2:
            rate = processed_sec / max(elapsed, 1e-6)
            remain_audio = max(0.0, total_sec - processed_sec)
            est_remain = remain_audio / max(rate, 1e-6)
            finish_ts = time.strftime(
                "%Y-%m-%d %H:%M:%S", time.localtime(time.time() + est_remain)
            )
            eta_txt = f"ï½œé è¨ˆå‰©é¤˜ {fmt_dur(est_remain)}ï½œé è¨ˆå®Œæˆ {finish_ts}"
        prog.progress(
            min(1.0, processed_sec / total_sec),
            text=f"å·²è½‰éŒ„éŸ³æ™‚é•· {fmt_dur(processed_sec)} / {fmt_dur(total_sec)}ï¼Œè€—æ™‚ {fmt_dur(elapsed)}{eta_txt}",
        )



    if punc_rule:
        final_punc = SmartPunctuator(lexical_cfg)
        srt_chunks = []
        # Collect the raw SRT payload (text + timing) so punctuation can respect pauses
        for i in range(0, len(srt_lines), 4):
            if i + 2 >= len(srt_lines):
                continue
            time_line = srt_lines[i + 1]
            text_line = srt_lines[i + 2].strip()
            if not text_line:
                continue
            try:
                start_str, end_str = time_line.split("-->")
                start_sec = to_seconds(start_str.strip())
                end_sec = to_seconds(end_str.strip())
            except Exception:
                start_sec, end_sec = None, None
            srt_chunks.append((text_line, start_sec, end_sec))

        # SmartPunctuator è£œæ¨™é»
        for text, start, end in srt_chunks:
            text = normalize_zh_punc(apply_common_corrections(text))
            final_punc.add_chunk(text, start, end, punctuation_cfg.comma_threshold, punctuation_cfg.period_threshold)

        final_punc.ensure_terminal()


        # âœ… æ™ºèƒ½è£œæ¨™é»ï¼šåªæœ‰ã€Œç„¡ä»»ä½•çµ‚æ­¢ç¬¦ã€æ™‚æ‰è£œå¥è™Ÿ
        tokens_fixed = []
        for t in final_punc.tokens:
            t = t.rstrip()
            if not t:
                continue
            # è‹¥æœ«å°¾å·²æœ‰ä»»æ„æ¨™é»ç¬¦è™Ÿï¼ˆåŒ…å«é€—è™Ÿã€é “è™Ÿã€å†’è™Ÿç­‰ï¼‰ï¼Œå‰‡ä¸é‡è¤‡è£œå¥è™Ÿ
            if t[-1] not in "ã€‚ï¼ï¼Ÿâ€¦ï¼Œã€ï¼›ï¼š":
                t += "ã€‚"
            tokens_fixed.append(t)



        # æ¯å¥æ›è¡Œè¼¸å‡ºï¼ˆæ¯å¥å·²åŠ æ¨™é»ï¼‰
        final_text = "\n".join(tokens_fixed)
        final_text = normalize_zh_punc(final_text)

    else:
        # æ²’é–‹æ¨™é»è¦å‰‡å°±å–®ç´” join
        final_text = "".join(
            normalize_zh_punc(apply_common_corrections(line.strip()))
            for i, line in enumerate(srt_lines)
            if i % 4 == 2
        )
        if final_text and final_text[-1] not in "ã€‚ï¼ï¼Ÿ":
            final_text += "ã€‚"

    if 'reflow_enable' in globals() and reflow_enable:
        final_text = reflow_text(final_text, max_len=reflow_width)


    elapsed = time.time() - t0
    stats_area.info(f"å·²è½‰éŒ„éŸ³æ™‚é•·ï¼š{fmt_dur(processed_sec)}ï¼›è€—æ™‚ï¼š{fmt_dur(elapsed)}")

    return final_text, "\n".join(srt_lines), processed_sec, elapsed



if not IS_WORKER:
    # ==== è¼¸å…¥ ====
    uploaded = None
    uploaded_name = None
    uploaded = st.file_uploader("ä¸Šå‚³éŸ³æª”", type=["m4a","mp3","wav","flac"])
    if uploaded is not None:
        uploaded_name = uploaded.name

        # è¨­å®šä¸Šå‚³ç‹€æ…‹
        st.session_state['file_uploaded'] = True

        # åŠ å…¥é˜²èª¤é—œé–‰æç¤º
        st.components.v1.html(
            """
            <script>
            if (!window.hasUploadWarning) {
                window.onbeforeunload = function(e) {
                    return 'è½‰å¯«å°šæœªå®Œæˆï¼Œç¢ºå®šè¦é›¢é–‹å—ï¼Ÿ';
                };
                window.hasUploadWarning = true;
            }
            </script>
            """,
            height=0,
        )

    # é¡¯ç¤º/ä¸‹è¼‰å€
    status = st.empty()
    top_info = st.empty()
    live_cols = st.columns(2)


    # === é¡¯ç¤ºæ¨¡å¼æ§åˆ¶ï¼ˆæ”¾åœ¨æ•´åˆé¡¯ç¤ºå€æœ€ä¸Šé¢ï¼‰ ===

    # åˆå§‹åŒ–ç‹€æ…‹
    transcribing = st.session_state.get("transcribing", False)

    # é¡¯ç¤ºç°åŒ–æ¨£å¼ + ç¦ç”¨æ¸¸æ¨™
    st.markdown(
        """
        <style>
        div[data-testid="stHorizontalBlock"] label[aria-disabled="true"] {
            opacity: 0.4 !important;
            pointer-events: none !important;
            cursor: not-allowed !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # é¡¯ç¤ºæ¨¡å¼é¸é …
    show_line_mode = st.radio(
        "é¡¯ç¤ºæ¨¡å¼",
        ["æ¯å¥æ›è¡Œ", "æ•´æ®µé¡¯ç¤º"],
        index=st.session_state.get("show_line_mode_idx", 0),
        key="show_line_mode",
        horizontal=True,
        disabled=transcribing,  # è‹¥æ­£åœ¨è½‰éŒ„å‰‡ç¦ç”¨
        help="åˆ‡æ›é¡¯ç¤ºæ–¹å¼ï¼Œä¸å½±éŸ¿ä¸‹è¼‰å…§å®¹",
    )

    if transcribing:
        st.caption("ğŸ”’ è½‰éŒ„é€²è¡Œä¸­ï¼Œæš«æ™‚ç„¡æ³•åˆ‡æ›é¡¯ç¤ºæ¨¡å¼ã€‚")

    # === é¡¯ç¤ºçµæœå€ ===
    final_box = st.empty()

    status_message = st.session_state.get("transcribe_status_message")
    success_message = st.session_state.get("transcribe_success_message")
    if success_message:
        status.success(success_message)
    elif status_message:
        status.info(status_message)

    # è‹¥æœ‰å¿«å–çµæœï¼ˆä¸Šæ¬¡è½‰éŒ„ï¼‰ï¼Œè‡ªå‹•è¼‰å…¥
    last_txt = st.session_state.get("last_txt")
    if last_txt:
        line_break_version = re.sub(r"\s*([ã€‚ï¼ï¼Ÿ])\s*", r"\1\n", last_txt)
        line_break_version = re.sub(r"\n+", "\n", line_break_version).strip()
        display_text = (
            line_break_version
            if show_line_mode == "æ¯å¥æ›è¡Œ"
            else last_txt.replace("\n", " ")
        )





# === é–å®šé¡¯ç¤ºæ¨¡å¼èˆ‡ä¸»æµç¨‹ ===
if not IS_WORKER and st.session_state.get("start_transcribe_pending"):
    if uploaded is None:
        status.warning("è«‹ä¸Šå‚³éŸ³è¨Šæª”å†é–‹å§‹è½‰å¯«")
        st.session_state["transcribing"] = False
        st.session_state["start_transcribe_pending"] = False
        st.session_state["transcribe_status_message"] = ""
        st.session_state["transcribe_success_message"] = ""
        if "run_button_primary" in st.session_state:
            del st.session_state["run_button_primary"]
        st.stop()

    st.session_state["transcribe_status_message"] = "âš™ï¸ æ­£åœ¨æº–å‚™è¼‰å…¥æ¨¡å‹â€¦"
    status.info(st.session_state["transcribe_status_message"])

    # --- å¾é€™è£¡é–‹å§‹é€²å…¥ä¸»æµç¨‹ ---
    st.session_state["transcribing"] = True

    # æº–å‚™è‡¨æ™‚ç›®éŒ„
    tmp_dir = os.getcwd()
    ts = int(time.time())

    # === è¨­å®šæœ¬æ¬¡ run çš„æš«å­˜/å¢é‡å„²å­˜è·¯å¾‘ï¼ˆä»¥ timestamp å€åˆ†ï¼‰ ===
    base_filename = os.path.splitext(uploaded_name)[0]
    txt_path_run = os.path.join(tmp_dir, f"{base_filename}_{ts}.txt")
    srt_path_run = os.path.join(tmp_dir, f"{base_filename}_{ts}.srt")
    st.session_state["current_txt_path"] = txt_path_run
    st.session_state["current_srt_path"] = srt_path_run
    st.session_state["current_base_filename"] = base_filename
    st.session_state["current_ts"] = ts

    # å–å¾— bytes èˆ‡æ™‚é•·
    raw = uploaded.read()
    seg, total_sec = audio_info_from_bytes(raw, uploaded_name)

    upload_ts = time.strftime("%Y-%m-%d %H:%M:%S")
    top_info.info(f"ä¸Šå‚³éŒ„éŸ³æ™‚é•·ï¼š{fmt_dur(total_sec)}ï½œä¸Šå‚³æ™‚é–“é»ï¼š{upload_ts}")

    # è½‰ç‚ºå–®ä¸€ wav æª”ï¼ˆä½œç‚ºå¾ŒçºŒåˆ‡æ®µåŸºç¤ï¼‰
    base_path = os.path.join(tmp_dir, f"_tmp_{ts}.wav")
    export_seg_to_wav(seg.set_channels(1).set_frame_rate(44100).set_sample_width(2), base_path)

    # æ§‹å»ºåˆå§‹æç¤ºï¼ˆåˆä½µä½¿ç”¨è€…æç¤ºèˆ‡é ˜åŸŸè©å½™ï¼‰
    combined_prompt = (init_prompt or "").strip()
    domain_prompt_preview = ""
    domain_terms: List[str] = []
    candidate_domain_files = combined_domain_files if 'combined_domain_files' in globals() else []
    if 'auto_use_domain' in globals() and auto_use_domain and candidate_domain_files:
        status.info("æ­£åœ¨æ“·å–é ˜åŸŸè©å½™â€¦")
        st.session_state["transcribe_status_message"] = "æ­£åœ¨æ“·å–é ˜åŸŸè©å½™â€¦"
        try:
            domain_prompt_preview, domain_terms = build_domain_prompt(candidate_domain_files, top_k=terms_topk)
        except Exception:
            domain_prompt_preview = ""
            domain_terms = []
        if domain_prompt_preview:
            combined_prompt = (combined_prompt + ("ï¼Œ" if combined_prompt else "") + domain_prompt_preview).strip("ï¼Œ ")
            with st.sidebar:
                st.caption("å·²å¸¶å…¥é ˜åŸŸè©å½™ï¼ˆå‰ 800 å­—å…§ï¼‰ï¼š")
                st.code(domain_prompt_preview[:400] + ("â€¦" if len(domain_prompt_preview) > 400 else ""))

    # è¼‰å…¥æ¨¡å‹
    status.info(f"è¼‰å…¥æ¨¡å‹ï¼š{model_name} / {compute} / beam={beam_size}")
    st.session_state["transcribe_status_message"] = f"è¼‰å…¥æ¨¡å‹ï¼š{model_name} / {compute} / beam={beam_size}"
    model = load_model(model_name, compute)

    # === åˆ‡æ®µèˆ‡ä¸¦è¡Œç­–ç•¥ ===
    device_now = st.session_state.get("device_used", "cpu")
    overlap = 5.0

    def make_chunks(total_s: float, k: int):
        """Create k roughly equal segments with slight overlap to protect word boundaries."""
        step = total_s / max(1, k)
        out = []
        for i in range(k):
            start = max(0.0, i*step - (overlap/2 if i>0 else 0.0))
            end = min(total_s, (i+1)*step + (overlap/2 if i<k-1 else 0.0))
            out.append((start, end))
        return out

    # æ˜¯å¦å•Ÿç”¨è‡ªå‹•åˆ†æ®µï¼šé è¨­ç‚ºå–®æ®µè™•ç†ï¼Œä»¥é™ä½éŒ¯èª¤é¢¨éšª
    if not auto_chunk_audio:
        k = 1
    else:
        # Metal ä¸Šå–®æ®µæœ€å„ªï¼›CPU è¦–é•·åº¦åˆ‡ 3~5 æ®µ
        if device_now == "metal":
            k = 1
        else:
            if total_sec <= 20*60:
                k = 1
            elif total_sec <= 40*60:
                k = 3
            elif total_sec <= 60*60:
                k = 4
            else:
                k = 5

    chunks = make_chunks(total_sec, k)

    # åŒ¯å‡ºæ¯æ®µ wav
    chunk_paths = []
    for i,(a,b) in enumerate(chunks, 1):
        p = os.path.join(tmp_dir, f"_tmp_{ts}_{i}.wav")
        export_seg_to_wav(seg[a*1000:b*1000], p)
        chunk_paths.append((p,a,b))

    device_msg = f"è£ç½®ï¼š{device_now}ï¼›åˆ‡æ®µ {k}ï¼ˆå« {overlap}s é‡ç–Šï¼‰ï¼›ä¸¦è¡Œï¼šå¦"
    combined_status = f"{device_msg}ï½œğŸ“ æ­£åœ¨è½‰éŒ„éŸ³æª”â€¦"
    status.info(combined_status)
    st.session_state["transcribe_status_message"] = combined_status

    final_texts, final_srts = [], []
    total_elapsed = 0.0

    if k == 1:
        # å–®æ®µå³æ™‚é¡¯ç¤º
        with st.container():
            st.subheader("è½‰å¯«é€²åº¦")
            one_live = st.empty()
            one_prog = st.empty()
            one_stats = st.empty()
        seg_start_ts = time.strftime("%Y-%m-%d %H:%M:%S")
        st.caption(f"åˆ†æ®µ 1 é–‹å§‹è½‰æ›æ™‚é–“é»ï¼š{seg_start_ts}")
        status.info("é–‹å§‹è½‰å¯«â€¦")
        st.session_state["transcribe_status_message"] = "é–‹å§‹è½‰å¯«â€¦"
        final_text, final_srt, _, elapsed = transcribe_one(
            os.path.join(tmp_dir, f"_tmp_{ts}_1.wav"), model,
            language="zh", vad_filter=vad, beam_size=beam_size,
            initial_prompt=combined_prompt, punc_rule=punc_rule,
            vad_parameters=vad_params,
            ui_area=one_live, progress_area=one_prog, stats_area=one_stats,
            time_offset_sec=chunks[0][0], total_sec_for_progress=(chunks[0][1]-chunks[0][0]),
            lexical_rules=LEXICAL_RULES,
            punc_settings=CURRENT_PUNCT_SETTINGS,
        )
        total_elapsed = elapsed
        final_texts.append(final_text)
        final_srts.append(final_srt)
    else:
        # é †åºåˆ†æ®µè™•ç†ï¼ˆä¸å˜—è©¦ä¸¦è¡Œï¼‰
        st.subheader("é †åºåˆ†æ®µè™•ç†")
        bar = st.progress(0.0, text="åˆå§‹åŒ–â€¦")
        span_total = sum(b-a for _,a,b in chunk_paths)
        span_done = 0.0
        t0 = time.time()
        for i,(p,a,b) in enumerate(chunk_paths, 1):
            seg_start_ts = time.strftime("%Y-%m-%d %H:%M:%S")
            st.caption(f"åˆ†æ®µ {i} é–‹å§‹è½‰æ›æ™‚é–“é»ï¼š{seg_start_ts}")
            part_live = st.empty(); part_prog = st.empty(); part_stats = st.empty()
            text_i, srt_i, _, _ = transcribe_one(
                p, model, language="zh", vad_filter=vad, beam_size=beam_size,
                initial_prompt=combined_prompt, punc_rule=punc_rule,
                vad_parameters=vad_params,
                ui_area=part_live, progress_area=part_prog, stats_area=part_stats,
                time_offset_sec=a, total_sec_for_progress=(b-a),
                lexical_rules=LEXICAL_RULES,
                punc_settings=CURRENT_PUNCT_SETTINGS,
            )
            final_texts.append(text_i)
            final_srts.append(srt_i)
            span_done += (b - a)
            elapsed = time.time() - t0
            # ä»¥ç›®å‰å¹³å‡é€Ÿç‡ä¼°è¨ˆå‰©é¤˜ï¼ˆæ•´é«”ï¼‰
            done_ratio = max(1e-6, span_done / max(span_total, 1e-6))
            eta_txt = ""
            if done_ratio > 0.02:
                est_total = elapsed / done_ratio
                est_remain = max(0.0, est_total - elapsed)
                finish_ts = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time() + est_remain))
                eta_txt = f"ï½œé è¨ˆå‰©é¤˜ {fmt_dur(est_remain)}ï½œé è¨ˆå®Œæˆ {finish_ts}"
            bar.progress(min(1.0, span_done/span_total), text=f"å®Œæˆ {i}/{k} æ®µï¼›è€—æ™‚ {fmt_dur(elapsed)}{eta_txt}")
        total_elapsed = time.time() - t0

    # åˆä½µ

    # ğŸŸ¢æ›¿æ›å¾Œé–‹å§‹
    # === æœ€çµ‚æ•´åˆéšæ®µ ===
    # åˆä½µæ‰€æœ‰åˆ†æ®µ SRTï¼ˆä¾›ä¸‹è¼‰èˆ‡æ™‚é–“è»¸ä½¿ç”¨ï¼‰
    final_srt = "\n".join(final_srts)

    # ç›´æ¥åˆä½µæ¯æ®µçš„æœ€çµ‚æ–‡å­—ï¼ˆé€™äº›æ–‡å­—åœ¨ transcribe_one è£¡å·²åŠ å¥½æ¨™é»èˆ‡æ›è¡Œï¼‰
    final_text = "\n".join(final_texts)

    # è‹¥é–‹å•Ÿ reflowï¼Œå†çµ±ä¸€é‡æ’
    if 'reflow_enable' in globals() and reflow_enable:
        final_text = reflow_text(final_text, max_len=reflow_width)

    # === è‡ªå‹•ä¿å­˜åˆ°æœ¬åœ° ===
    # è‡ªå‹•ä¿å­˜åˆ°æœ¬åœ°
    if uploaded_name:
        base_filename = st.session_state.get("current_base_filename") or os.path.splitext(uploaded_name)[0]
        txt_path = os.path.join(tmp_dir, f"{base_filename}.txt")
        srt_path = os.path.join(tmp_dir, f"{base_filename}.srt")
        with open(txt_path, "w", encoding="utf-8") as f_txt:
            f_txt.write(final_text)
        with open(srt_path, "w", encoding="utf-8") as f_srt:
            f_srt.write(final_srt)
        st.sidebar.success(f"å·²è‡ªå‹•ä¿å­˜åˆ°æœ¬åœ°ï¼š{txt_path} å’Œ {srt_path}")



    # å»é™¤æ¨™é»å¾Œå¤šé¤˜ç©ºç™½ï¼Œå†æ›è¡Œ
    line_break_version = re.sub(r"\s*([ã€‚ï¼ï¼Ÿ])\s*", r"\1\n", final_text)
    line_break_version = re.sub(r"\n+", "\n", line_break_version).strip()

    if show_line_mode == "æ¯å¥æ›è¡Œ":
        display_text = line_break_version
    else:
        display_text = final_text.replace("\n", " ")



    # === é¡¯ç¤ºçµæœèˆ‡è€—æ™‚ ===
    final_box.markdown(display_text.replace("\n", "  \n"))
    success_msg = f"å®Œæˆï¼›ç¸½è€—æ™‚ {fmt_dur(total_elapsed)}"
    st.session_state["transcribe_success_message"] = success_msg

    # === ç‹€æ…‹é‡ç½® ===
    st.session_state["transcribing"] = False
    st.session_state["start_transcribe_pending"] = False
    st.session_state["transcribe_status_message"] = ""

    # å®‰å…¨é‡ç½®æŒ‰éˆ•ç‹€æ…‹
    if "run_button_primary" in st.session_state:
        del st.session_state["run_button_primary"]

    # === å¿«å–çµæœ ===
    st.session_state["last_txt"] = final_text
    st.session_state["last_srt"] = final_srt

    # === æ¸…ç†æš«å­˜æª” ===
    for p_tuple in [(base_path, None, None)] + chunk_paths:
        try:
            os.remove(p_tuple[0])
        except Exception:
            pass

    _trigger_rerun()


    # === çµ±ä¸€é¡¯ç¤ºå€ï¼ˆç„¡è«–æ˜¯å¦å‰›è½‰å®ŒéŒ„éŸ³ï¼‰ ===
if not IS_WORKER and st.session_state.get("last_txt"):

    st.markdown("---")
    st.subheader("è½‰éŒ„çµæœ")

    # === ä¸‹è¼‰æŒ‰éˆ•ï¼ˆä¿æŒé¡¯ç¤ºï¼‰ ===
    with st.expander("ğŸ“¥ ä¸‹è¼‰æª”æ¡ˆ", expanded=True):
        dl_cols = st.columns(2)
        new_txt = st.session_state["last_txt"].encode("utf-8")
        new_srt = st.session_state.get("last_srt", "").encode("utf-8")
        with dl_cols[0]:
            base_name = st.session_state.get("current_base_filename", "transcript")
            ts_val = st.session_state.get("current_ts", "")
            fname_txt = f"{base_name}_{ts_val}.txt" if ts_val else f"{base_name}.txt"
            st.download_button("ä¸‹è¼‰ TXT", data=new_txt, file_name=fname_txt, key="dl_txt")
        with dl_cols[1]:
            base_name = st.session_state.get("current_base_filename", "transcript")
            ts_val = st.session_state.get("current_ts", "")
            fname_srt = f"{base_name}_{ts_val}.srt" if ts_val else f"{base_name}.srt"
            st.download_button("ä¸‹è¼‰ SRT", data=new_srt, file_name=fname_srt, key="dl_srt")

    # æ ¹æ“šé¡¯ç¤ºæ¨¡å¼æ¸²æŸ“å…§å®¹
    cached_text = st.session_state["last_txt"]
    line_break_version = re.sub(r"\s*([ã€‚ï¼ï¼Ÿ])\s*", r"\1\n", cached_text)
    line_break_version = re.sub(r"\n+", "\n", line_break_version).strip()
    display_text = (
        line_break_version
        if st.session_state.get("show_line_mode", "æ¯å¥æ›è¡Œ") == "æ¯å¥æ›è¡Œ"
        else cached_text.replace("\n", " ")
    )

    st.markdown(display_text.replace("\n", "  \n"))

    # === å´é‚Šæ¬„ä¸€éµè¤‡è£½ ===
    with st.sidebar:
        show_line_mode = st.session_state.get("show_line_mode", "æ¯å¥æ›è¡Œ")
        text_to_copy = (
            re.sub(r"\s*([ã€‚ï¼ï¼Ÿ])\s*", r"\1\n", st.session_state["last_txt"])
            if show_line_mode == "æ¯å¥æ›è¡Œ"
            else st.session_state["last_txt"].replace("\n", " ")
        )
        if st.button("ğŸ“‹ é¡¯ç¤ºå¯è¤‡è£½å…§å®¹", key="copy_btn"):
            st.session_state["copy_buffer"] = text_to_copy
            st.toast("å·²é¡¯ç¤ºå¯è¤‡è£½å…§å®¹ï¼Œè«‹åœ¨ä¸‹æ–¹å€å¡Šæ‰‹å‹•è¤‡è£½ã€‚", icon="ğŸ“‹")

        if "copy_buffer" in st.session_state:
            st.text_area("ç›®å‰é¡¯ç¤ºå…§å®¹ï¼ˆå¯å…¨é¸å¾Œè¤‡è£½ï¼‰", st.session_state["copy_buffer"], height=300)
